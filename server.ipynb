{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntlWazLZd4t",
        "outputId": "9e235cd8-5599-4a00-849a-cd336d41a1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-M68LD0_1mREYHzhVaxpy8n0soXaUK0n\n",
            "To: /content/Приказ Минтранса России от 23 06 2022 N 250.docx\n",
            "\r  0% 0.00/5.30M [00:00<?, ?B/s]\r100% 5.30M/5.30M [00:00<00:00, 92.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1-M68LD0_1mREYHzhVaxpy8n0soXaUK0n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "034MdUhSLcGn",
        "outputId": "cc5a55cc-b76b-4b83-cc8e-6b720160e81c"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers openpyxl\\\n",
        "accelerate langchain docx2txt chromadb bitsandbytes peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wIAYYXg8UTRQ"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import transformers\n",
        "import peft\n",
        "import torch\n",
        "\n",
        "from tqdm.autonotebook import tqdm as notebook_tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PHoxn3BEn0f"
      },
      "source": [
        "## Embedding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nQf0ICZXmGPq"
      },
      "outputs": [],
      "source": [
        "embed_model_id = 'intfloat/multilingual-e5-base'\n",
        "\n",
        "device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-BE-hJw1TheO"
      },
      "outputs": [],
      "source": [
        "raw_documents = Docx2txtLoader('Приказ Минтранса России от 23 06 2022 N 250.docx').load()\n",
        "text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "documents = text_splitter.split_documents(raw_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wrEo4sREWj5L"
      },
      "outputs": [],
      "source": [
        "db = Chroma.from_documents(documents, embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nmWkfc3ocTYq"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_type=\"mmr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-mvRiArEvOu"
      },
      "source": [
        "## LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "74472662b17e47099e945de8eefd5961",
            "44ecb912f3e140f2b815eb02e08fb015",
            "c65e152b43504661a858b2649159b8db",
            "13149da63be74d59bd15bf8576c3b4d3",
            "19c9f561886b4c6e9600c252ff982cf6",
            "df58216a2c074d08b89fa4800cbda8f8",
            "89d1039207f547259a9352c297a97872",
            "5a6c8467e3cd46c6b232006a68298088",
            "1f2aef1e50bf44928138bdd3ad600063",
            "5cba091a13744ccda80fb3756a69401f",
            "d4eef28d8c03444db6d13478cb2e318c"
          ]
        },
        "id": "G8XL2ZjS8btX",
        "outputId": "e6fc63e5-bb73-4b14-fd90-bb7602f75fbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74472662b17e47099e945de8eefd5961",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ],
      "source": [
        "model_id = \"IlyaGusev/saiga2_13b_lora\"\n",
        "\n",
        "hf_auth = 'TOKEN'\n",
        "\n",
        "model_config = peft.PeftConfig.from_pretrained(model_id)\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_config.base_model_name_or_path,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = peft.PeftModel.from_pretrained(\n",
        "    model,\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16\n",
        ").eval()\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v0iPv1GDGxgT"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAYXi8ayKusU",
        "outputId": "c7c2cc58-c586-427d-e138-2ce75ba8c240"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1,\n",
        "    num_beams=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVu2KHaMLM2M"
      },
      "source": [
        "## RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-8RxQYwHRg0N"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T72l2uOsCwq0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "        llm=llm, chain_type='stuff',\n",
        "        retriever=retriever\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG0Ei3HrvPVC",
        "outputId": "ac9b62a2-a588-45ea-8b88-7d02453a6109"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7ab410685d80>)), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7ab463d326b0>, search_type='mmr'))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yi3k5IeHvRTy"
      },
      "outputs": [],
      "source": [
        "question_template = \"system:\\n\"\\\n",
        "                    \"Используй следующие фрагменты контекста, чтобы ответить на вопрос в конце. \"\\\n",
        "                    \"Если ты не знаешь ответа, просто скажи, что не знаешь что ответить, не пытайся придумать ответ.\"\\\n",
        "                    \"\\n\\n{context}\\n\\nВопрос: {question}\\nОтвет: \"\n",
        "\n",
        "rag_pipeline.combine_documents_chain.llm_chain.prompt.template = question_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJzAqu0SvSES",
        "outputId": "a3af2b6f-3d69-4f76-addb-487e737d4f0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='system:\\nИспользуй следующие фрагменты контекста, чтобы ответить на вопрос в конце. Если ты не знаешь ответа, просто скажи, что не знаешь что ответить, не пытайся придумать ответ.\\n\\n{context}\\n\\nВопрос: {question}\\nОтвет: '), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7ab410685d80>)), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7ab463d326b0>, search_type='mmr'))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5LsJAgC9xo"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcnMsx9kE3a8"
      },
      "source": [
        "## Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXwD330RmBKK",
        "outputId": "780d3c2e-4429-45fa-abbd-e2265f6bc20f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'Кто должен выполнять правила технической эксплуатации?',\n",
              " 'result': ' Владельцы инфраструктуры (владельцы железнодорожных путей необщего пользования) и дежурные по железнодорожным станциям.'}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_pipeline('Кто должен выполнять правила технической эксплуатации?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A0WedWgDGGM",
        "outputId": "b201b9c7-46cd-4cde-986b-beb14bd9707d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'На ком ответственность за содержание путей?',\n",
              " 'result': ' Ответственность за содержание путей лежит на владельце железнодорожных путей необщего пользования.'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_pipeline('На ком ответственность за содержание путей?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABqjZgklDMU5",
        "outputId": "f534c43e-b516-4c49-a2d5-11daba2fd189"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'Кому разрешен доступ на локомотивы?',\n",
              " 'result': ' Разрешение на доступ на локомотивы может быть предоставлено только владельцем инфраструктуры (владельцем железнодорожных путей необщего пользования).'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_pipeline('Кому разрешен доступ на локомотивы?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EzOO2dX7Cna3"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfPgbSKFmcAb",
        "outputId": "a63b3577-6ede-4643-9be3-aa81f890676b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nJXpYdurC1q8jvGwoDggWPrI4aWb6Roi\n",
            "To: /content/output.xlsx\n",
            "\r  0% 0.00/352k [00:00<?, ?B/s]\r100% 352k/352k [00:00<00:00, 171MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1nJXpYdurC1q8jvGwoDggWPrI4aWb6Roi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ErgY38ZTnZSt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel('output.xlsx')\n",
        "data = data[['question', 'answer_summary', \"answers_merged\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpjH8yOLokG8",
        "outputId": "60554e63-0bec-4632-a080-5c66e3e3df0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               question  \\\n",
            "1022  Как выглядит подталкивающий локомотив или спец...   \n",
            "\n",
            "                                         answer_summary  \\\n",
            "1022  Подталкивающий локомотив и специальный самоход...   \n",
            "\n",
            "                                         answers_merged  \n",
            "1022  ['Подталкивающий локомотив и самоходный специа...  \n",
            "1022 Как выглядит подталкивающий локомотив или специальный самоходный подвижной состав?\n"
          ]
        }
      ],
      "source": [
        "random_row = data.sample()\n",
        "print(random_row)\n",
        "index = random_row.index[0]\n",
        "name_value = random_row['question'].iloc[0]\n",
        "print(index, name_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hAacxPoOkbLk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "\n",
        "def get_question(data):\n",
        "    shape = data.shape[0]\n",
        "    index = random.randint(0, shape)\n",
        "    question = data.iloc[index]['question']\n",
        "    return index, question\n",
        "\n",
        "\n",
        "def init_model(model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_score_from_answers(model, question_id, user_answer):\n",
        "    expected_answer_summary = data.iloc[question_id]['answer_summary']\n",
        "    expected_answers = eval(data.iloc[question_id]['answers_merged'])\n",
        "    embeddings1 = model.encode([user_answer], convert_to_tensor=True)\n",
        "    embeddings2 = model.encode(expected_answers, convert_to_tensor=True)\n",
        "    cosine_score = float(util.cos_sim(embeddings1, embeddings2).max().item())\n",
        "    print(user_answer, *expected_answers, cosine_score, sep='\\n')\n",
        "    return cosine_score, expected_answer_summary\n",
        "\n",
        "\n",
        "message_template = \"{role}\\n{content}\\n\"\n",
        "\n",
        "def answerUsingKnowledgeBase(question):\n",
        "    answer = rag_pipeline(question)\n",
        "    return answer['result'].strip()\n",
        "\n",
        "\n",
        "def answerUsingHistory(history_json : list[dict]):\n",
        "    history = \"\"\n",
        "\n",
        "    for message in history_json:\n",
        "        message_text = message_template.format(**message)\n",
        "        history += message_text\n",
        "\n",
        "    answer = llm(history)\n",
        "\n",
        "    return answer.strip()\n",
        "\n",
        "\n",
        "def generateAnswer(history_json : list[dict]):\n",
        "    assert len(history_json) >= 2\n",
        "\n",
        "    if len(history_json) == 2:\n",
        "        question = history_json[-1][\"content\"]\n",
        "        return answerUsingKnowledgeBase(question)\n",
        "\n",
        "    return answerUsingHistory(history_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcG0u5Ijpv7I",
        "outputId": "56ac7d54-76ab-4a9e-e437-fd8f1f15400d"
      },
      "outputs": [],
      "source": [
        "!pip install uvicorn nest_asyncio fastapi pyngrok kaleido python-multipart pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcLxS89SwhLV",
        "outputId": "07f8657a-3efa-4bed-b648-d200abc76cf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EknUf4upIrl",
        "outputId": "cc30c8b2-f342-472f-e499-be26991ac962"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-11-12T06:45:31+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n",
            "INFO:     Started server process [37365]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://patient-buck-weekly.ngrok-free.app\n",
            "[{'role': 'system', 'content': 'Используй следующие фрагменты контекста, чтобы ответить на вопрос в конце. Если ты не знаешь ответа, просто скажи, что не знаешь что ответить, не пытайся придумать ответ'}, {'role': 'user', 'content': 'Какие требования предъявляются к размещению сооружений и устройств вагонного хозяйства?'}]\n",
            "INFO:     188.64.15.38:0 - \"POST /get_answer HTTP/1.1\" 200 OK\n",
            "[{'role': 'system', 'content': 'Используй следующие фрагменты контекста, чтобы ответить на вопрос в конце. Если ты не знаешь ответа, просто скажи, что не знаешь что ответить, не пытайся придумать ответ'}, {'role': 'user', 'content': 'Какие требования предъявляются к размещению сооружений и устройств вагонного хозяйства?'}, {'role': 'bot', 'content': 'Требования к размещению сооружений и устройств вагонного хозяйства определяются правилами технического обслуживания железнодорожного транспорта и другими нормативными документами. Они включают в себя требования к расположению зданий и сооружений, размещению технологических устройств, ограждению территории, организации коммуникаций и других аспектов.'}, {'role': 'user', 'content': 'Что такое вагонное хозяйство?'}]\n",
            "INFO:     188.64.15.38:0 - \"POST /get_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "Она для данного типа светофоров устанавливается в соответствии с требованиями для основного светофора\n",
            "Видимость сигнальных огней повторительных светофоров (в совокупности с основным) обеспечивается на расстоянии, установленном требованиями для данного типа светофоров (основного).\n",
            "Она для данного типа светофоров установливается в соответствии с требованиями для основного светофора.\n",
            "0.9912205338478088\n",
            "0.9912205338478088 Она для данного типа светофоров устанавливается в соответствии с требованиями для основного светофора\n",
            "INFO:     188.64.15.38:0 - \"POST /check_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "Снегоочистители, движущиеся в голове состава по неправильному пути на двухпутных участках, обозначаются желтыми флагами или огнями фонарей в светлое и темное время суток. С левой стороны должен быть красный флаг или огонь фонаря, а в сторону локомотива направляются три прозрачно-белых контрольных огня\n",
            "Снегоочистители при движении их в голове по неправильному железнодорожному пути на двухпутных участках обозначаются: в светлое время суток - два желтых развернутых флага и красный развернутый флаг под желтым слева на боковых крюках; в темное время суток - соответственно два желтых и один красный огни фонарей, а в сторону локомотива - три прозрачно-белых контрольных огня (рисунок 202).\n",
            "Снегоочистители, движущиеся в голове состава по неправильному пути на двухпутных участках, обозначаются желтыми флагами или огнями фонарей в светлое и темное время суток. С левой стороны должен быть красный флаг или огонь фонаря, а в сторону локомотива направляются три прозрачно-белых контрольных огня.\n",
            "0.9989283084869385\n",
            "0.9989283084869385 Снегоочистители, движущиеся в голове состава по неправильному пути на двухпутных участках, обозначаются желтыми флагами или огнями фонарей в светлое и темное время суток. С левой стороны должен быть красный флаг или огонь фонаря, а в сторону локомотива направляются три прозрачно-белых контрольных огня\n",
            "INFO:     188.64.15.38:0 - \"POST /check_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "Наружный рельс возвышается на 20 см\n",
            "На кривых участках пути независимо от радиуса кривой возвышение наружного рельса не должно превышать 150 мм.\n",
            "На кривых участках пути возвышение наружного рельса должно быть не более 150 мм.\n",
            "0.6455861330032349\n",
            "0.6455861330032349 Наружный рельс возвышается на 20 см\n",
            "INFO:     188.64.15.38:0 - \"POST /check_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "[{'role': 'system', 'content': 'Используй следующие фрагменты контекста, чтобы ответить на вопрос в конце. Если ты не знаешь ответа, просто скажи, что не знаешь что ответить, не пытайся придумать ответ'}, {'role': 'user', 'content': 'Было два козла, сколько?'}]\n",
            "INFO:     188.64.15.38:0 - \"POST /get_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "разные виды\n",
            "На железнодорожных станциях в зависимости от технологической потребности применяются устройства станционной радиосвязи, устройства двусторонней парковой связи (на основе радиосвязи, или громкоговорящей связи, или их сочетания), ремонтно-оперативная радиосвязь, беспроводная (радиосвязь) передачи данных для информационно-управляющих систем и другие виды технологической электросвязи.\n",
            "Станционная радиосвязь, двусторонняя парковая связь (на основе радиосвязи, громкоговорящей связи или их сочетания), ремонтно-оперативная радиосвязь, беспроводная связь и другие виды.\n",
            "0.1252545267343521\n",
            "0.1252545267343521 разные виды\n",
            "INFO:     188.64.15.38:0 - \"POST /check_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "твыфдлмывм\n",
            "Перечень объектов железнодорожного транспорта, принадлежащих владельцу инфраструктуры (владельцу железнодорожных путей необщего пользования), подлежащих оборудованию системами и устройствами отображения времени, устанавливается локальным нормативным актом владельца инфраструктуры (владельца железнодорожных путей необщего пользования).\n",
            "Локальный нормативный акт владельца инфраструктуры определяет перечень объектов, которые подлежат оборудованию устройствами отображения времени.\n",
            "0.07108329236507416\n",
            "0.07108329236507416 твыфдлмывм\n",
            "INFO:     188.64.15.38:0 - \"POST /check_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "фымвфылтждвмфыжвмт\n",
            "Видимость сигнальных огней выходных и маршрутных светофоров главных железнодорожных путей составляет не менее 400 м, выходных и маршрутных светофоров главных железнодорожных путей в кривых, боковых железнодорожных путей, горочных светофоров, пригласительных сигналов и маневровых светофоров - не менее 200 м, а показания маршрутных указателей - не менее 100 м.\n",
            "Не менее 400 м.\n",
            "0.32502782344818115\n",
            "0.32502782344818115 фымвфылтждвмфыжвмт\n",
            "INFO:     188.64.15.38:0 - \"POST /check_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "фыдлвомтлдыфовтмфывм\n",
            "В процессе эксплуатации пассажирских платформ с номинальными значениями высоты 1300, 1100, 550 мм и расстояния от оси железнодорожного пути 1920 мм, и пассажирских платформ с номинальными значениями высоты 200 мм и расстояния от оси железнодорожного пути 1745 мм допускается изменение установленных габаритом приближения строений расстояний от оси железнодорожного пути до 30 мм в сторону увеличения и до 25 мм в сторону уменьшения.\n",
            "При эксплуатации пассажирских платформ номинальной высотой 1300, 1100, 550 мм и расстоянием от оси 1920 мм, а также платформ номинальной высотой 200 мм и расстоянием от оси 1745 мм, допускается увеличение установленных габаритом приближения строений расстояний от оси пути максимум на 30 мм и уменьшение максимум на 25 мм.\n",
            "0.07936331629753113\n",
            "0.07936331629753113 фыдлвомтлдыфовтмфывм\n",
            "INFO:     188.64.15.38:0 - \"POST /check_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n",
            "флыовтмлдыфтвлмотфыдлвм\n",
            "Расстояние от нижней точки проводов воздушных линий электропередачи напряжением свыше 1000 В до поверхности земли при максимальной стреле провеса должно быть не менее: 6,0 м - на перегонах, в том числе 5,0 м - в труднодоступных местах; 7,0 м - на пересечениях с автомобильными дорогами, железнодорожных станциях и в населенной местности.\n",
            "Не менее: 6,0 м - на перегонах, в том числе 5,0 м - в труднодоступных местах; 7,0 м - на пересечениях с автомобильными дорогами, станциях и в населенной местности.\n",
            "0.12735925614833832\n",
            "0.12735925614833832 флыовтмлдыфтвлмотфыдлвм\n",
            "INFO:     188.64.15.38:0 - \"POST /check_answer HTTP/1.1\" 200 OK\n",
            "INFO:     188.64.15.38:0 - \"GET /get_rand_question HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import time\n",
        "import re\n",
        "\n",
        "from fastapi import FastAPI, File, UploadFile, Request, Response, HTTPException, Form\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from fastapi.responses import FileResponse\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "from PIL import Image\n",
        "\n",
        "from typing import List, Dict\n",
        "\n",
        "testing_model = init_model()\n",
        "\n",
        "class Question(BaseModel):\n",
        "    context: List[Dict[str, str]]\n",
        "\n",
        "\n",
        "class AnswerFromUser(BaseModel):\n",
        "    question_id: int\n",
        "    answer: str\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "@app.get('/')\n",
        "async def main(request: Request):\n",
        "    return {\"message\": \"Hello World\"}\n",
        "\n",
        "\n",
        "@app.get('/get_rand_question')\n",
        "async def get_rand_question(request: Request):\n",
        "    question_id, question = get_question(data)\n",
        "    responce = {\n",
        "        \"question_id\": question_id,\n",
        "        \"question\": question\n",
        "    }\n",
        "    return responce\n",
        "\n",
        "\n",
        "@app.post(\"/get_answer\")\n",
        "async def get_answer(question: Question):\n",
        "    answer = generateAnswer(question.context)\n",
        "    print(question.context)\n",
        "    return answer\n",
        "\n",
        "\n",
        "@app.post(\"/check_answer\")\n",
        "async def check_answer(answer_from_user: AnswerFromUser):\n",
        "    question_id = answer_from_user.question_id\n",
        "    user_answer = answer_from_user.answer\n",
        "    answer_coef, expected_answer = get_score_from_answers(testing_model, question_id, user_answer)\n",
        "    print(answer_coef, user_answer, )\n",
        "    response = {\n",
        "        \"answer_coef\": answer_coef,\n",
        "        \"expected_answer\": expected_answer\n",
        "    }\n",
        "    return response\n",
        "\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000,  domain=\"patient-buck-weekly.ngrok-free.app\")\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13149da63be74d59bd15bf8576c3b4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cba091a13744ccda80fb3756a69401f",
            "placeholder": "​",
            "style": "IPY_MODEL_d4eef28d8c03444db6d13478cb2e318c",
            "value": " 3/3 [02:04&lt;00:00, 38.78s/it]"
          }
        },
        "19c9f561886b4c6e9600c252ff982cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f2aef1e50bf44928138bdd3ad600063": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44ecb912f3e140f2b815eb02e08fb015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df58216a2c074d08b89fa4800cbda8f8",
            "placeholder": "​",
            "style": "IPY_MODEL_89d1039207f547259a9352c297a97872",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5a6c8467e3cd46c6b232006a68298088": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cba091a13744ccda80fb3756a69401f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74472662b17e47099e945de8eefd5961": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44ecb912f3e140f2b815eb02e08fb015",
              "IPY_MODEL_c65e152b43504661a858b2649159b8db",
              "IPY_MODEL_13149da63be74d59bd15bf8576c3b4d3"
            ],
            "layout": "IPY_MODEL_19c9f561886b4c6e9600c252ff982cf6"
          }
        },
        "89d1039207f547259a9352c297a97872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65e152b43504661a858b2649159b8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a6c8467e3cd46c6b232006a68298088",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f2aef1e50bf44928138bdd3ad600063",
            "value": 3
          }
        },
        "d4eef28d8c03444db6d13478cb2e318c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df58216a2c074d08b89fa4800cbda8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
